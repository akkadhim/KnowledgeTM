{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a60e61d-4946-4f95-845c-d4d3c329fb23",
   "metadata": {},
   "source": [
    "# Phase 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d0d45c-b002-4f32-ae57-90eced7d651f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-12 07:31:42,488 - tmu.util.cuda_profiler - WARNING - Could not import pycuda: cuInit failed: no CUDA-capable device is detected\n",
      "2024-06-12 07:31:42,507 - tmu.clause_bank.clause_bank_cuda - ERROR - cuInit failed: no CUDA-capable device is detected\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/tmu/clause_bank/clause_bank_cuda.py\", line 43, in <module>\n",
      "    import pycuda.autoinit\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pycuda/autoinit.py\", line 5, in <module>\n",
      "    cuda.init()\n",
      "pycuda._driver.RuntimeError: cuInit failed: no CUDA-capable device is detected\n",
      "2024-06-12 07:31:42,508 - tmu.clause_bank.clause_bank_cuda - WARNING - Could not import pycuda. This indicates that it is not installed! A possible fix is to run 'pip install pycuda'. Fallback to CPU ClauseBanks.\n",
      "2024-06-12 07:31:42,913 - numexpr.utils - INFO - Note: detected 96 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "2024-06-12 07:31:42,914 - numexpr.utils - INFO - Note: NumExpr detected 96 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "2024-06-12 07:31:42,914 - numexpr.utils - INFO - NumExpr defaulting to 8 threads.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Epochs:   7%|▋         | 7/100 [2:07:04<27:59:20, 1083.45s/it]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from contextlib import redirect_stdout\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "from collections import defaultdict\n",
    "from tmu.models.autoencoder.autoencoder import TMAutoEncoder\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from Evaluation import Evaluation\n",
    "from Tools import Tools\n",
    "from DirectoriesUtil import Dicrectories\n",
    "\n",
    "target_similarity=defaultdict(list)\n",
    "clause_weight_threshold = 0\n",
    "clause_drop_p = 0.0\n",
    "factor = 20\n",
    "clauses = 80\n",
    "T = factor*40\n",
    "s = 5.0\n",
    "epochs = 100\n",
    "number_of_examples = 1000\n",
    "accumulation = 10\n",
    "sub_accumulation = 10\n",
    "top_max_clauses1 = 0\n",
    "top_max_clauses2 = 0\n",
    "with_clause_update = False\n",
    "max_spearman = 0.9\n",
    "true_weight = 0.7\n",
    "false_weight = 1 - true_weight\n",
    "neg_length = 50\n",
    "\n",
    "eval = Evaluation()\n",
    "def preprocess_text(text):\n",
    "    return text\n",
    "vectorizer_X = Tools.read_pickle_data(\"big_vectorizer_X.pickle\")\n",
    "feature_names = vectorizer_X.get_feature_names_out()\n",
    "number_of_features = vectorizer_X.get_feature_names_out().shape[0]\n",
    "\n",
    "for dataset_name in os.listdir(Dicrectories.datasets):\n",
    "    if dataset_name == 'rg-65':\n",
    "        current_folder_path = os.path.join(Dicrectories.datasets, dataset_name)\n",
    "        if os.path.isdir(current_folder_path):\n",
    "            files_start_name = os.path.join(current_folder_path, dataset_name)\n",
    "\n",
    "            pair_list = Tools.get_dataset_pairs(files_start_name)\n",
    "            output_active, target_words = Tools.get_dataset_targets(files_start_name)\n",
    "            \n",
    "            result_filepath = Dicrectories.test(dataset_name,\"all_phase2\")\n",
    "            with open(result_filepath, 'w') as file, redirect_stdout(file):\n",
    "                tm = TMAutoEncoder(clauses, T, s, output_active, max_included_literals=3, accumulation=accumulation, feature_negation=False, platform='CPU', output_balancing=0.5)\n",
    "                total_training = 0\n",
    "                print(\"Epochs: %d\" % epochs)\n",
    "                print(\"Target words: %d\" % len(target_words))\n",
    "                print(\"No of features: %d\" % number_of_features)\n",
    "                print(\"Clauses: %d\" % clauses)\n",
    "                print(\"with_clause_update: %s\" % with_clause_update)\n",
    "                print(\"Examples: %d\" % number_of_examples)\n",
    "                print(\"Accumulation: %d\" % accumulation)\n",
    "                print(\"Sub Accumulation: %d\" % sub_accumulation)\n",
    "                print(\"true_weight: %f\" % true_weight)\n",
    "                print(\"false_weight: %f\" % false_weight)\n",
    "                print(\"top_max_clauses1: %d\" % top_max_clauses1)\n",
    "                print(\"top_max_clauses2: %d\\n\" % top_max_clauses2)\n",
    "                \n",
    "                epochs_progress_bar = tqdm(total=epochs, desc=\"Running Epochs\")\n",
    "                for e in range(epochs):\n",
    "                    print(\"\\nEpoch #: %d\" % e)\n",
    "                    start_training = time()\n",
    "                    tm.knowledge_fit(\n",
    "                        number_of_examples = number_of_examples,\n",
    "                        number_of_features = number_of_features,\n",
    "                        sub_accumulation = sub_accumulation,\n",
    "                        top_max_clauses1 = top_max_clauses1,\n",
    "                        top_max_clauses2 = top_max_clauses2,\n",
    "                        neg_length = neg_length,\n",
    "                        with_clause_update = with_clause_update,\n",
    "                        true_weight = true_weight,\n",
    "                        false_weight = false_weight,\n",
    "                        print_c = False\n",
    "                        )\n",
    "                    stop_training = time()\n",
    "                    epoch_time = stop_training - start_training\n",
    "                    Tools.print_training_time(epoch_time)\n",
    "                    total_training = total_training + epoch_time\n",
    "\n",
    "                    profile = np.empty((len(target_words), clauses))\n",
    "                    for i in range(len(target_words)):\n",
    "                        weights = tm.get_weights(i)\n",
    "                        profile[i,:] = np.where(weights >= clause_weight_threshold, weights, 0)\n",
    "                    similarity = cosine_similarity(profile)\n",
    "                    for i in range(len(target_words)):\n",
    "                        sorted_index = np.argsort(-1*similarity[i,:])\n",
    "                        for j in range(1, len(target_words)):\n",
    "                            target_similarity[(target_words[i], target_words[sorted_index[j]])]  = similarity[i,sorted_index[j]]\n",
    "                    spearman = eval.calculate(target_similarity,pair_list)\n",
    "                    if spearman > max_spearman:\n",
    "                        break\n",
    "                    epochs_progress_bar.update(1)\n",
    "                epochs_progress_bar.close()\n",
    "\n",
    "                print(\"\\n=====================================\\nClauses\\n=====================================\")\n",
    "                for j in range(clauses):\n",
    "                    print(\"Clause #%-2d \" % (j), end=' ')\n",
    "                    for tw in range(len(target_words)):\n",
    "                        print(\"%s:W%-5d \" % (target_words[tw], tm.get_weight(tw, j)), end='| ')\n",
    "                    l = [] \n",
    "                    number_of_literals = 0 \n",
    "                    for k in range(tm.clause_bank.number_of_literals):\n",
    "                        if tm.get_ta_action(j, k) == 1:\n",
    "                            number_of_literals = number_of_literals + 1\n",
    "                            if k < tm.clause_bank.number_of_features:\n",
    "                                l.append(\"%s(%d)\" % (feature_names[k], tm.clause_bank.get_ta_state(j, k)))\n",
    "                            else:\n",
    "                                l.append(\"¬%s(%d)\" % (feature_names[k-tm.clause_bank.number_of_features], tm.clause_bank.get_ta_state(j, k)))\n",
    "                    print(\": No of features:%-6d\" % (number_of_literals), end=\" ==> \")\n",
    "                    try:\n",
    "                        print(\" - \".join(l))\n",
    "                    except UnicodeEncodeError:\n",
    "                        print(\" exception \")\n",
    "                \n",
    "                print(\"\\n=====================================\\nWord Similarity\\n=====================================\")\n",
    "                max_word_length = len(max(target_words, key=len))\n",
    "                list_of_words = []\n",
    "                target_words_with_min_max = []\n",
    "                for i in range(len(target_words)):\n",
    "                    row_of_similarity = []\n",
    "                    sorted_index = np.argsort(-1*similarity[i,:])\n",
    "                    min_similarity = 1.0\n",
    "                    max_similarity = 0.0\n",
    "                    word_similarity = []\n",
    "                    for j in range(1, len(target_words)):\n",
    "                        target_similarity[(target_words[i], target_words[sorted_index[j]])]  = similarity[i,sorted_index[j]]\n",
    "                        row_of_similarity.append(target_words[sorted_index[j]])\n",
    "                        word_similarity.append(\"{:<{}}({:.2f})  \".format(target_words[sorted_index[j]], max_word_length, similarity[i, sorted_index[j]]))\n",
    "                        if(min_similarity > similarity[i,sorted_index[j]]):\n",
    "                            min_similarity = similarity[i,sorted_index[j]]\n",
    "                        if(max_similarity < similarity[i,sorted_index[j]]):\n",
    "                            max_similarity = similarity[i,sorted_index[j]]\n",
    "                \n",
    "                    output_line = f\"{target_words[i]:<{max_word_length}}: Min:{min_similarity:.2f}, Max:{max_similarity:.2f}\"\n",
    "                    print(output_line, end='     ==> ')\n",
    "                    print(word_similarity)\n",
    "                    list_of_words.append(row_of_similarity)\n",
    "                    target_words_with_min_max.append(output_line)\n",
    "\n",
    "                Tools.print_training_time(total_training)\n",
    "                \n",
    "            eval = ResultHelper.get_file_max_spearman(result_filepath)\n",
    "            dir_name, old_file_name = os.path.split(result_filepath)\n",
    "            new_file_path = os.path.join(dir_name, \"{:.2f}\".format(eval) + \"_\"  + old_file_name)\n",
    "            os.rename(result_filepath, new_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6f44f7-b912-498c-9c6d-77ea956fbf51",
   "metadata": {},
   "source": [
    "# Pairs Phase 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "042380e7-69ed-4c5a-afa1-a890723ffc43",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Epochs:   1%|          | 1/100 [10:51<17:55:15, 651.67s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 83\u001b[0m\n\u001b[1;32m     79\u001b[0m tm \u001b[38;5;241m=\u001b[39m TMAutoEncoder(clauses, T, s, pair_output_active, max_included_literals\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, accumulation\u001b[38;5;241m=\u001b[39maccumulation, feature_negation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, platform\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCPU\u001b[39m\u001b[38;5;124m'\u001b[39m, output_balancing\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# profile = cProfile.Profile() \u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# profile.enable()\u001b[39;00m\n\u001b[0;32m---> 83\u001b[0m \u001b[43mtm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mknowledge_pair_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnumber_of_examples\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnumber_of_examples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnumber_of_features\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnumber_of_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43msub_accumulation\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msub_accumulation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_max_clauses1\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtop_max_clauses1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_max_clauses2\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtop_max_clauses2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwith_clause_update\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mwith_clause_update\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrue_weight\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrue_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfalse_weight\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfalse_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprint_c\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# profile.disable()\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# profile.print_stats(sort='time')\u001b[39;00m\n\u001b[1;32m     97\u001b[0m stop_training \u001b[38;5;241m=\u001b[39m time()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/tmu/models/autoencoder/autoencoder.py:429\u001b[0m, in \u001b[0;36mTMAutoEncoder.knowledge_pair_fit\u001b[0;34m(self, number_of_examples, number_of_features, sub_accumulation, top_max_clauses1, top_max_clauses2, with_clause_update, true_weight, false_weight, shuffle_classes, print_c)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (index \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_active):\n\u001b[1;32m    428\u001b[0m     next_tw \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_active[index \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m--> 429\u001b[0m     documents_of_features\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43msub_accumulation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_max_clauses1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_max_clauses2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mknowledge_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_tw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_value\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    430\u001b[0m \u001b[38;5;66;03m# save document to x\u001b[39;00m\n\u001b[1;32m    431\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclause_bank\u001b[38;5;241m.\u001b[39mproduce_autoencoder_knowledge(\n\u001b[1;32m    432\u001b[0m     number_of_features \u001b[38;5;241m=\u001b[39m number_of_features,\n\u001b[1;32m    433\u001b[0m     documents_of_features \u001b[38;5;241m=\u001b[39m documents_of_features,\n\u001b[1;32m    434\u001b[0m     enable_c_log \u001b[38;5;241m=\u001b[39m print_c\n\u001b[1;32m    435\u001b[0m )   \n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/tmu/models/autoencoder/autoencoder.py:463\u001b[0m, in \u001b[0;36mTMAutoEncoder.get_features\u001b[0;34m(self, sub_accumulation, top_max_clauses1, top_max_clauses2, knowledge_directory, tw, target_value)\u001b[0m\n\u001b[1;32m    461\u001b[0m     literal_filtered_clauses \u001b[38;5;241m=\u001b[39m [clause \u001b[38;5;28;01mfor\u001b[39;00m clause \u001b[38;5;129;01min\u001b[39;00m literal_all_clauses \u001b[38;5;28;01mif\u001b[39;00m clause[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 463\u001b[0m     literal_filtered_clauses \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mclause\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclause\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mliteral_all_clauses\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclause\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    465\u001b[0m literal_clauses_subset \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39msample(literal_filtered_clauses, sub_accumulation)\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(top_max_clauses2 \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/tmu/models/autoencoder/autoencoder.py:463\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    461\u001b[0m     literal_filtered_clauses \u001b[38;5;241m=\u001b[39m [clause \u001b[38;5;28;01mfor\u001b[39;00m clause \u001b[38;5;129;01min\u001b[39;00m literal_all_clauses \u001b[38;5;28;01mif\u001b[39;00m clause[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 463\u001b[0m     literal_filtered_clauses \u001b[38;5;241m=\u001b[39m [clause \u001b[38;5;28;01mfor\u001b[39;00m clause \u001b[38;5;129;01min\u001b[39;00m literal_all_clauses \u001b[38;5;28;01mif\u001b[39;00m clause[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    465\u001b[0m literal_clauses_subset \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39msample(literal_filtered_clauses, sub_accumulation)\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(top_max_clauses2 \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from contextlib import redirect_stdout\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "from collections import defaultdict\n",
    "from tmu.models.autoencoder.autoencoder import TMAutoEncoder\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from Evaluation import Evaluation\n",
    "from Tools import Tools\n",
    "from DirectoriesUtil import Dicrectories\n",
    "\n",
    "import cProfile\n",
    "\n",
    "clause_weight_threshold = 0\n",
    "clause_drop_p = 0.0\n",
    "factor = 4\n",
    "T = factor*40\n",
    "clauses = int(factor*30/(1.0 - clause_drop_p))\n",
    "s = 5.0\n",
    "epochs = 100\n",
    "number_of_examples = 100\n",
    "accumulation = 10\n",
    "sub_accumulation = 14\n",
    "top_max_clauses1 = 0\n",
    "top_max_clauses2 = 0\n",
    "with_clause_update = False\n",
    "true_weight = 0.6\n",
    "false_weight = 1 - true_weight\n",
    "\n",
    "eval = Evaluation()\n",
    "def preprocess_text(text):\n",
    "    return text\n",
    "vectorizer_X = Tools.read_pickle_data(\"vectorizer_X.pickle\")\n",
    "feature_names = vectorizer_X.get_feature_names_out()\n",
    "number_of_features = vectorizer_X.get_feature_names_out().shape[0]\n",
    "\n",
    "for dataset_name in os.listdir(Dicrectories.datasets):\n",
    "    if dataset_name == 'rg-65':\n",
    "        current_folder_path = os.path.join(Dicrectories.datasets, dataset_name)\n",
    "        if os.path.isdir(current_folder_path):\n",
    "            files_start_name = os.path.join(current_folder_path, dataset_name)\n",
    "            pair_list = Tools.get_dataset_pairs(files_start_name)\n",
    "            output_active, target_words = Tools.get_dataset_targets(files_start_name)\n",
    "            available_pair_list = []\n",
    "            pairs_output_active = []\n",
    "            for pair, score in pair_list:\n",
    "                word1, word2 = pair[0], pair[1]\n",
    "                if all(word in target_words for word in [word1, word2]):\n",
    "                    available_pair_list.append([pair,score])\n",
    "                    pairs_output_active.append([vectorizer_X.vocabulary_[word1], vectorizer_X.vocabulary_[word2]])\n",
    "            \n",
    "            result_filepath = Dicrectories.test(dataset_name,\"pair_pair_phase2\")\n",
    "            with open(result_filepath, 'w') as file, redirect_stdout(file):\n",
    "                total_training = 0\n",
    "                print(\"Epochs: %d\" % epochs)\n",
    "                print(\"Target words: %d\" % len(target_words))\n",
    "                print(\"No of features: %d\" % number_of_features)\n",
    "                print(\"Clauses: %d\" % clauses)\n",
    "                print(\"with_clause_update: %s\" % with_clause_update)\n",
    "                print(\"Examples: %d\" % number_of_examples)\n",
    "                print(\"Accumulation: %d\" % accumulation)\n",
    "                print(\"Sub Accumulation: %d\" % sub_accumulation)\n",
    "                print(\"true_weight: %f\" % true_weight)\n",
    "                print(\"false_weight: %f\" % false_weight)\n",
    "                print(\"top_max_clauses1: %d\" % top_max_clauses1)\n",
    "                print(\"top_max_clauses2: %d\\n\" % top_max_clauses2)\n",
    "                \n",
    "                epochs_progress_bar = tqdm(total=epochs, desc=\"Running Epochs\")\n",
    "                for e in range(epochs):\n",
    "                    print(\"\\nEpoch #: %d\" % e)\n",
    "                    epoch_time = 0\n",
    "                    target_similarity=defaultdict(list)\n",
    "                    for pair_index, pair in enumerate(pairs_output_active):\n",
    "                        pair_output_active = np.empty(2, dtype=np.uint32)\n",
    "                        pair_output_active[0] = pair[0]\n",
    "                        pair_output_active[1] = pair[1]\n",
    "                        start_training = time()\n",
    "                        tm = TMAutoEncoder(clauses, T, s, pair_output_active, max_included_literals=3, accumulation=accumulation, feature_negation=False, platform='CPU', output_balancing=0.5)\n",
    "                        \n",
    "                        # profile = cProfile.Profile() \n",
    "                        # profile.enable()\n",
    "                        tm.knowledge_pair_fit(\n",
    "                            number_of_examples = number_of_examples,\n",
    "                            number_of_features = number_of_features,\n",
    "                            sub_accumulation = sub_accumulation,\n",
    "                            top_max_clauses1 = top_max_clauses1,\n",
    "                            top_max_clauses2 = top_max_clauses2,\n",
    "                            with_clause_update = with_clause_update,\n",
    "                            true_weight = true_weight,\n",
    "                            false_weight = false_weight,\n",
    "                            print_c = False\n",
    "                            )\n",
    "                        # profile.disable()\n",
    "                        # profile.print_stats(sort='time')\n",
    "\n",
    "                        stop_training = time()\n",
    "                        pair_time = stop_training - start_training\n",
    "                        epoch_time = epoch_time + pair_time\n",
    "\n",
    "                        profile = np.empty((2, clauses))\n",
    "                        weights = tm.get_weights(0)\n",
    "                        profile[0,:] = np.where(weights >= clause_weight_threshold, weights, 0)\n",
    "                        weights = tm.get_weights(1)\n",
    "                        profile[1,:] = np.where(weights >= clause_weight_threshold, weights, 0)\n",
    "                        similarity = cosine_similarity(profile)\n",
    "\n",
    "                        sorted_index = np.argsort(-1*similarity[0,:])\n",
    "                        target_similarity[available_pair_list[pair_index][0]]  = similarity[0,sorted_index[1]]\n",
    "\n",
    "                    Tools.print_training_time(epoch_time)\n",
    "                    total_training = total_training + epoch_time\n",
    "                    eval.calculate(target_similarity,available_pair_list)\n",
    "                    epochs_progress_bar.update(1)\n",
    "                epochs_progress_bar.close()\n",
    "                Tools.print_training_time(total_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3603c231-0eac-424f-bae7-a1be8a625aac",
   "metadata": {},
   "source": [
    "# Check cached files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "29e7424c-4ddf-44a4-b90c-3bc56b43400a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cached results: 2600\n"
     ]
    }
   ],
   "source": [
    "from Tools import Tools\n",
    "num_cached = Tools.read_pickle_data.cache_info().currsize\n",
    "print(\"Number of cached results:\", num_cached)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2670a6d-752d-4003-bdc9-86e9fe5e9d1d",
   "metadata": {},
   "source": [
    "# Print Phase2 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "47ad7588-2816-46f1-88f3-98b2ab4a4c02",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forest\n",
      "['land', 'farm', 'new', 'water', 'site', 'water', 'strip', 'rover', 'air', 'area', 'time', 'value', 'rover', 'space', 'construction', 'government', 'area', 'property', 'property', 'world', 'property', 'water', 'rover', 'burned', 'carbon', 'forest', 'coal', 'fire', 'black', 'wednesday', 'body', 'boy', 'ground', 'police', 'fire', 'water', 'family', 'fire', 'house', 'ground', 'death', 'death', 'people', 'blaze', 'death', 'put', 'understand', 'river', 'basin', 'new', 'pearl', 'bridge', 'people', 'valley', 'bridge', 'new', 'bridge', 'feet', 'flood', 'bridge', 'park', 'fish', 'north', 'red', 'area', 'water', 'year', 'north', 'water', 'new', 'plane', 'lawn', 'park', 'white', 'front', 'grass', 'park', 'tennis', 'house', 'park', 'south', 'association', 'tennis', 'tennis', 'white', 'garden', 'large', 'garden', 'week', 'white', 'front', 'week', 'association', 'club', 'house', 'association', 'tennis', 'garden', 'tennis', 'garden', 'south', 'rain', 'storm', 'weather', 'game', 'heavy', 'day', 'heavy', 'snow', 'self', 'forecast', 'heavy', 'year', 'course', 'wednesday', 'wind', 'heavy', 'wednesday', 'new', 'play', 'storm', 'weather', 'heavy', 'second', 'hit', 'snow', 'great', 'snow', 'area', 'gauge', 'wednesday', 'park', 'army', 'city', 'killed', 'city', 'military', 'new', 'property', 'province', 'development', 'responsible', 'border', 'people', 'west', 'downtown', 'village', 'park', 'event', 'hotel', 'street', 'school', 'south', 'show', 'river', 'school', 'state', 'rock', 'red', 'crowd', 'site', 'city', 'south', 'hall', 'city', 'town', 'council', 'apartment', 'executive', 'meeting', 'town', 'association', 'city', 'number', 'guardian', 'security', 'year', 'community', 'support', 'city', 'discuss', 'city', 'north', 'people', 'seat', 'russia', 'town', 'city', 'member', 'security', 'tax', 'land', 'food', 'year', 'water', 'cattle', 'area', 'new', 'water', 'rover', 'airport', 'population', 'river', 'dark', 'protection', 'new', 'property', 'aircraft', 'plane', 'farm', 'land', 'holy', 'new', 'water', 'border', 'law', 'department', 'palestinians', 'new', 'sea', 'area', 'israel', 'state', 'palestinians', 'account', 'agreement', 'transport', 'desert', 'park', 'west', 'new', 'police', 'south', 'center', 'car', 'city', 'car', 'college', 'amusement', 'man', 'north', 'forest', 'car', 'street', 'city', 'new', 'old', 'home', 'new', 'north', 'car', 'open', 'place', 'city', 'court', 'drive', 'number', 'commission', 'region', 'market', 'people', 'court', 'new', 'customers', 'people', 'recent', 'rose', 'drop', 'decade', 'large', 'year', 'large', 'new', 'time', 'children', 'new', 'people', 'club', 'new']\n",
      "279\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from Tools import Tools\n",
    "from DirectoriesUtil import Dicrectories\n",
    "\n",
    "number_of_examples = 1\n",
    "accumulation = 10\n",
    "sub_accumulation = 14\n",
    "top_max_clauses1 = 0\n",
    "top_max_clauses2 = 0\n",
    "first= True\n",
    "\n",
    "for dataset_name in os.listdir(Dicrectories.datasets):\n",
    "    if dataset_name == 'rg-65':\n",
    "        current_folder_path = os.path.join(Dicrectories.datasets, dataset_name)\n",
    "        if os.path.isdir(current_folder_path):\n",
    "            files_start_name = os.path.join(current_folder_path, dataset_name)\n",
    "\n",
    "            pair_list = Tools.get_dataset_pairs(files_start_name)\n",
    "            output_active, target_words = Tools.get_dataset_targets(files_start_name)\n",
    "            # for op in output_active:\n",
    "                # print(vectorizer_X.get_feature_names_out()[op])\n",
    "            \n",
    "            number_of_clases = len(output_active)\n",
    "            class_index = np.arange(number_of_clases, dtype=np.uint32)\n",
    "            knowledge_directory = Dicrectories.knowledge\n",
    "\n",
    "            avg_per_ex_features = 0\n",
    "            documents_of_features = []\n",
    "            for ex in range(number_of_examples):\n",
    "                rng = np.random.RandomState(None)\n",
    "                rng.shuffle(class_index)\n",
    "                num_features = 0\n",
    "                for index in class_index:\n",
    "                    tw = output_active[index]\n",
    "                    target_value = random.randint(0, 1)\n",
    "    \n",
    "                    tw_knowledge_path = Dicrectories.pickle_by_id(knowledge_directory , tw)\n",
    "                    tw_all_clauses = Tools.read_pickle_data(tw_knowledge_path)\n",
    "                    if target_value == 1:\n",
    "                        tw_filtered_clauses = [clause for clause in tw_all_clauses if clause[0] > 0]\n",
    "                    else:\n",
    "                        tw_filtered_clauses = [clause for clause in tw_all_clauses if clause[0] < 0]\n",
    "    \n",
    "                    tw_clauses_subset = random.sample(tw_filtered_clauses, accumulation)\n",
    "                    if(top_max_clauses1 > 0):\n",
    "                        tw_clauses_subset = sorted(tw_clauses_subset, key=lambda x: x[0], reverse=True)[:top_max_clauses1]\n",
    "    \n",
    "                    for tw_clause in tw_clauses_subset:\n",
    "                        related_literals = tw_clause[1]\n",
    "                        for literal in related_literals:\n",
    "                            documents_of_features.append(literal)\n",
    "                            literal_knowledge_path = Dicrectories.pickle_by_id(knowledge_directory , literal)\n",
    "                            literal_all_clauses = Tools.read_pickle_data(literal_knowledge_path)\n",
    "                            if target_value == 1:\n",
    "                                literal_filtered_clauses = [clause for clause in literal_all_clauses if clause[0] > 0]\n",
    "                            else:\n",
    "                                literal_filtered_clauses = [clause for clause in literal_all_clauses if clause[0] < 0]\n",
    "                            \n",
    "                            literal_clauses_subset = random.sample(literal_filtered_clauses, sub_accumulation)\n",
    "                            if(top_max_clauses2 > 0):\n",
    "                                literal_clauses_subset = sorted(literal_clauses_subset, key=lambda x: x[0], reverse=True)[:top_max_clauses2]\n",
    "    \n",
    "                            for literal_clause in literal_clauses_subset:\n",
    "                                literals = literal_clause[1]\n",
    "                                for sub_literal in literals:\n",
    "                                    documents_of_features.append(sub_literal)\n",
    "                    if first == True:\n",
    "                        first = False\n",
    "                        words = []\n",
    "                        print(vectorizer_X.get_feature_names_out()[tw])\n",
    "                        for feature in documents_of_features:\n",
    "                            words.append(vectorizer_X.get_feature_names_out()[feature])\n",
    "                        print(words)\n",
    "                        break\n",
    "                            \n",
    "                    # print(len(documents_of_features))\n",
    "                    # documents_of_features is all features will include for one class\n",
    "                    num_features = num_features + len(documents_of_features)\n",
    "                # avg_per_ex_features = avg_per_ex_features + (num_features / number_of_clases)\n",
    "                # print(num_features / number_of_clases)\n",
    "            # print(avg_per_ex_features/number_of_examples)\n",
    "            print(len(documents_of_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7229fc1-4f11-4d6c-8a0e-95b513d2a53c",
   "metadata": {},
   "source": [
    "# Check target_values balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16263780-a07b-407d-971d-b9ca199e2cd9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of True values: 1700\n",
      "Number of False values: 1700\n",
      "[True, False, False, True, True, False, True, True, True, False, False, True, False, True, False, True, True, False, False, True, False, True, False, False, True, False, True, False, False, True, False, True, True, False, True, False, True, True, False, True, True, False, False, False, False, False, False, False, False, True, False, True, True, True, False, True, True, True, True, True, False, True, True, False, True, False, True, False, False, True, True, True, True, False, True, True, True, True, False, False, True, False, False, True, False, True, False, False, True, True, False, False, False, False, False, True, False, False, False, True]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "from directories import Dicrectories\n",
    "from tools import Tools\n",
    "import numpy as np\n",
    "\n",
    "number_of_examples = 100\n",
    "accumulation = 10\n",
    "sub_accumulation = 14\n",
    "top_max_clauses1 = 0\n",
    "top_max_clauses2 = 0\n",
    "first= True\n",
    "target_values =[]\n",
    "target_valuess = random.choices([True, False], weights=[0.5, 0.5], k=number_of_examples)\n",
    "\n",
    "for dataset_name in os.listdir(Dicrectories.datasets):\n",
    "    if dataset_name == 'rg-65':\n",
    "        current_folder_path = os.path.join(Dicrectories.datasets, dataset_name)\n",
    "        if os.path.isdir(current_folder_path):\n",
    "            files_start_name = os.path.join(current_folder_path, dataset_name)\n",
    "\n",
    "            pair_list = Tools.get_dataset_pairs(files_start_name + '.csv')\n",
    "            output_active, target_words = Tools.get_targets(files_start_name)\n",
    "            \n",
    "            number_of_clases = len(output_active)\n",
    "            class_index = np.arange(number_of_clases, dtype=np.uint32)\n",
    "\n",
    "            avg_per_ex_features = 0\n",
    "            documents_of_features = []\n",
    "            for ex in range(number_of_examples):\n",
    "                rng = np.random.RandomState(None)\n",
    "                rng.shuffle(class_index)\n",
    "                num_features = 0\n",
    "                for index in class_index:\n",
    "                    tw = output_active[index]\n",
    "                    target_value = target_valuess[ex]\n",
    "                    target_values.append(target_value)\n",
    "\n",
    "num_true = sum(value == True for value in target_values)\n",
    "num_false = sum(value == False for value in target_values)\n",
    "\n",
    "print(f\"Number of True values: {num_true}\")\n",
    "print(f\"Number of False values: {num_false}\")\n",
    "print(target_valuess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "741ccc28-bfbd-4d9f-b8b0-e99687c87749",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of True values: 110\n",
      "Number of False values: 90\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "output_balancing = 0.5\n",
    "number_of_features = 2000\n",
    "rng = np.random.RandomState(None)\n",
    "num_samples = 200\n",
    "\n",
    "target_values =[]\n",
    "output_active = random.sample(range(num_samples + 1), num_samples)\n",
    "# print(output_active)\n",
    "X = csr_matrix((1, number_of_features), dtype=np.int64)\n",
    "\n",
    "feature_true_probability = np.ones(X.shape[1], dtype=np.float32) * output_balancing\n",
    "for op in output_active:\n",
    "    target_true_p = feature_true_probability[op]\n",
    "    target_value = rng.random() <= target_true_p\n",
    "    target_values.append(target_value)\n",
    "\n",
    "num_true = sum(value == True for value in target_values)\n",
    "num_false = sum(value == False for value in target_values)\n",
    "\n",
    "print(f\"Number of True values: {num_true}\")\n",
    "print(f\"Number of False values: {num_false}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325dacba-dc0a-45cc-9fbe-f30bd5292226",
   "metadata": {},
   "source": [
    "# Find positive and negative for tw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f59ee9a5-6d72-4f14-9bd1-0f40405bf166",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target word =  appeal\n",
      "Target word positive features =  ['crown', 'last', 'said', 'case', 'judge', 'said', 'told', 'said', 'supreme', 'told', 'high', 'said', 'supreme', 'heard', 'ruled', 'said', 'ballots', 'election', 'campaign', 'clinton', 'ballots', 'election', 'said', 'candidates', 'party', 'said', 'one', 'percent', 'registered', 'said', 'require', 'hoped', 'said', 'hoped', 'hoped', 'predicted', 'crown', 'last', 'said', 'case', 'judge', 'said', 'told', 'said', 'supreme', 'told', 'high', 'said', 'supreme', 'heard', 'ruled', 'said', 'said', 'saves', 'saves', 'debut', 'debut', 'mistakes', 'spokeswoman', 'spokeswoman', 'spokeswoman', 'spokeswoman', 'analyst', 'ballots', 'election', 'campaign', 'clinton', 'ballots', 'election', 'said', 'candidates', 'party', 'said', 'one', 'percent', 'registered', 'said', 'spokeswoman', 'spokeswoman', 'spokeswoman', 'spokeswoman', 'analyst', 'crown', 'last', 'said', 'case', 'judge', 'said', 'told', 'said', 'supreme', 'told', 'high', 'said', 'supreme', 'heard', 'ruled', 'said', 'board', 'said', 'final', 'justice', 'appeal', 'made', 'making', 'said', 'final', 'president', 'said', 'spokeswoman', 'spokeswoman', 'spokeswoman', 'spokeswoman', 'analyst', 'crown', 'last', 'said', 'case', 'judge', 'said', 'told', 'said', 'supreme', 'told', 'high', 'said', 'supreme', 'heard', 'ruled', 'said', 'claims', 'deal', 'said', 'would', 'said', 'suggestions', 'us', 'calls', 'saying', 'claims', 'mr', 'said', 'suggestions', 'spokeswoman', 'spokeswoman', 'spokeswoman', 'spokeswoman', 'analyst']\n",
      "Target word =  appeal\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 92\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget word = \u001b[39m\u001b[38;5;124m\"\u001b[39m,vectorizer_X\u001b[38;5;241m.\u001b[39mget_feature_names_out()[tw])\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m neg_words:\n\u001b[0;32m---> 92\u001b[0m     words\u001b[38;5;241m.\u001b[39mappend(\u001b[43mvectorizer_X\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_feature_names_out\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[feature])\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget word negative features = \u001b[39m\u001b[38;5;124m\"\u001b[39m,words)\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:1487\u001b[0m, in \u001b[0;36mCountVectorizer.get_feature_names_out\u001b[0;34m(self, input_features)\u001b[0m\n\u001b[1;32m   1473\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get output feature names for transformation.\u001b[39;00m\n\u001b[1;32m   1474\u001b[0m \n\u001b[1;32m   1475\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1483\u001b[0m \u001b[38;5;124;03m    Transformed feature names.\u001b[39;00m\n\u001b[1;32m   1484\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1485\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_vocabulary()\n\u001b[1;32m   1486\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39masarray(\n\u001b[0;32m-> 1487\u001b[0m     [t \u001b[38;5;28;01mfor\u001b[39;00m t, i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocabulary_\u001b[38;5;241m.\u001b[39mitems(), key\u001b[38;5;241m=\u001b[39mitemgetter(\u001b[38;5;241m1\u001b[39m))],\n\u001b[1;32m   1488\u001b[0m     dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mobject\u001b[39m,\n\u001b[1;32m   1489\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "from Tools import Tools\n",
    "from DirectoriesUtil import Dicrectories\n",
    "import numpy as np\n",
    "\n",
    "def preprocess_text(text):\n",
    "    return text\n",
    "number_of_examples = 1\n",
    "accumulation = 10\n",
    "sub_accumulation = 14\n",
    "top_max_clauses1 = 5\n",
    "top_max_clauses2 = 5\n",
    "first= True\n",
    "vectorizer_X = Tools.read_pickle_data(\"big_vectorizer_X.pickle\")\n",
    "feature_names = vectorizer_X.get_feature_names_out()\n",
    "number_of_features = vectorizer_X.get_feature_names_out().shape[0]\n",
    "\n",
    "for dataset_name in os.listdir(Dicrectories.datasets):\n",
    "    if dataset_name == 'rg-65':\n",
    "        current_folder_path = os.path.join(Dicrectories.datasets, dataset_name)\n",
    "        if os.path.isdir(current_folder_path):\n",
    "            files_start_name = os.path.join(current_folder_path, dataset_name)\n",
    "\n",
    "            pair_list = Tools.get_dataset_pairs(files_start_name)\n",
    "            output_active, target_words = Tools.get_dataset_targets(files_start_name)\n",
    "            # for op in output_active:\n",
    "                # print(vectorizer_X.get_feature_names_out()[op])\n",
    "            \n",
    "            number_of_clases = len(output_active)\n",
    "            class_index = np.arange(number_of_clases, dtype=np.uint32)\n",
    "            knowledge_directory = Dicrectories.knowledge\n",
    "\n",
    "            avg_per_ex_features = 0\n",
    "            documents_of_features = []\n",
    "            for ex in range(number_of_examples):\n",
    "                rng = np.random.RandomState(None)\n",
    "                rng.shuffle(class_index)\n",
    "                num_features = 0\n",
    "\n",
    "                pos_words = []\n",
    "                neg_words = []\n",
    "                for index in class_index:\n",
    "                    tw = output_active[index]\n",
    "                    target_value = random.randint(0, 1)\n",
    "    \n",
    "                    tw_knowledge_path = Dicrectories.pickle_by_id(knowledge_directory , tw)\n",
    "                    tw_all_clauses = Tools.read_pickle_data(tw_knowledge_path)\n",
    "                    pos_tw_filtered_clauses = [clause for clause in tw_all_clauses if clause[0] > 0]\n",
    "                    pos_tw_clauses_subset = sorted(pos_tw_filtered_clauses, key=lambda x: x[0], reverse=True)[:top_max_clauses1]\n",
    "                    neg_tw_filtered_clauses = [clause for clause in tw_all_clauses if clause[0] < 0]\n",
    "                    neg_tw_clauses_subset = sorted(neg_tw_filtered_clauses, key=lambda x: x[0], reverse=True)[:top_max_clauses1]\n",
    "    \n",
    "                    for tw_clause in pos_tw_clauses_subset:\n",
    "                        related_literals = tw_clause[1]\n",
    "                        for literal in related_literals:\n",
    "                            literal_knowledge_path = Dicrectories.pickle_by_id(knowledge_directory , literal)\n",
    "                            literal_all_clauses = Tools.read_pickle_data(literal_knowledge_path)\n",
    "                            literal_filtered_clauses = [clause for clause in literal_all_clauses if clause[0] > 0]\n",
    "                            pos_literal_clauses_subset = sorted(literal_filtered_clauses, key=lambda x: x[0], reverse=True)[:top_max_clauses2]\n",
    "                            \n",
    "                            for literal_clause in pos_literal_clauses_subset:\n",
    "                                literals = literal_clause[1]\n",
    "                                for sub_literal in literals:\n",
    "                                    pos_words.append(sub_literal)\n",
    "                                    \n",
    "                    for tw_clause in neg_tw_filtered_clauses:\n",
    "                        related_literals = tw_clause[1]\n",
    "                        for literal in related_literals:\n",
    "                            literal_knowledge_path = Dicrectories.pickle_by_id(knowledge_directory , literal)\n",
    "                            literal_all_clauses = Tools.read_pickle_data(literal_knowledge_path)\n",
    "                            literal_filtered_clauses = [clause for clause in literal_all_clauses if clause[0] < 0]\n",
    "                            neg_literal_clauses_subset = sorted(literal_filtered_clauses, key=lambda x: x[0], reverse=True)[:top_max_clauses2]\n",
    "                            \n",
    "                            for literal_clause in neg_literal_clauses_subset:\n",
    "                                literals = literal_clause[1]\n",
    "                                for sub_literal in literals:\n",
    "                                    neg_words.append(sub_literal)\n",
    "                                        \n",
    "                    if first == True:\n",
    "                        first = False\n",
    "                        \n",
    "                        words = []\n",
    "                        print(\"Target word = \",vectorizer_X.get_feature_names_out()[tw])\n",
    "                        for feature in pos_words:\n",
    "                            words.append(vectorizer_X.get_feature_names_out()[feature])\n",
    "                        print(\"Target word positive features = \",words)\n",
    "                        \n",
    "                        words = []\n",
    "                        print(\"Target word = \",vectorizer_X.get_feature_names_out()[tw])\n",
    "                        for feature in neg_words:\n",
    "                            words.append(vectorizer_X.get_feature_names_out()[feature])\n",
    "                        print(\"Target word negative features = \",words)\n",
    "                        \n",
    "                        break\n",
    "                            \n",
    "                    # print(len(documents_of_features))\n",
    "                    # documents_of_features is all features will include for one class\n",
    "                    num_features = num_features + len(documents_of_features)\n",
    "                # avg_per_ex_features = avg_per_ex_features + (num_features / number_of_clases)\n",
    "                # print(num_features / number_of_clases)\n",
    "            # print(avg_per_ex_features/number_of_examples)\n",
    "            print(len(documents_of_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "281c1800-feb1-47fb-ba78-59cc137b9e1f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "certificates: 37\n",
      "jittery: 37\n",
      "monks: 37\n",
      "wounds: 37\n",
      "rests: 37\n",
      "illustrator: 37\n",
      "ulrich: 37\n",
      "said: 23\n",
      "plate: 10\n",
      "pantomime: 10\n",
      "hobbit: 10\n",
      "unanimous: 10\n",
      "germain: 10\n",
      "two: 7\n",
      "blacktie: 7\n",
      "criminality: 7\n",
      "pathan: 7\n",
      "commandos: 7\n",
      "transformed: 7\n",
      "also: 7\n",
      "gastronomic: 5\n",
      "new: 5\n",
      "would: 5\n",
      "didnt: 5\n",
      "stephen: 5\n",
      "minsk: 5\n",
      "sindh: 5\n",
      "rochus: 5\n",
      "microfinance: 4\n",
      "complexes: 4\n",
      "starter: 4\n",
      "wood: 4\n",
      "cleveland: 4\n",
      "bens: 4\n",
      "nervy: 4\n",
      "akmal: 4\n",
      "comic: 4\n",
      "modernised: 4\n",
      "gasfired: 4\n",
      "cluster: 4\n",
      "christians: 4\n",
      "canals: 4\n",
      "rematch: 4\n",
      "one: 3\n",
      "percent: 3\n",
      "government: 3\n",
      "dioceses: 3\n",
      "newry: 3\n",
      "qualify: 3\n",
      "emea: 3\n",
      "grameen: 3\n",
      "mistrial: 3\n",
      "hastert: 3\n",
      "multistate: 3\n",
      "atwood: 3\n",
      "plunges: 3\n",
      "sunday: 3\n",
      "blue: 3\n",
      "mccullum: 3\n",
      "meaningfully: 3\n",
      "pga: 3\n",
      "inviting: 3\n",
      "course: 3\n",
      "holdings: 3\n",
      "nfc: 3\n",
      "cirque: 3\n",
      "titfortat: 3\n",
      "saddened: 3\n",
      "concludes: 3\n",
      "reel: 3\n",
      "repression: 3\n",
      "around: 3\n",
      "dear: 3\n",
      "refurbish: 3\n",
      "first: 3\n",
      "year: 3\n",
      "holtzeakin: 3\n",
      "peacemaking: 3\n",
      "measurement: 3\n",
      "police: 2\n",
      "exhibition: 2\n",
      "stanford: 2\n",
      "seminar: 2\n",
      "captured: 2\n",
      "study: 2\n",
      "shoes: 2\n",
      "vulcan: 2\n",
      "winstonsalem: 2\n",
      "tongues: 2\n",
      "industrywide: 2\n",
      "court: 2\n",
      "surfaces: 2\n",
      "ply: 2\n",
      "gerhard: 2\n",
      "armaments: 2\n",
      "joseph: 2\n",
      "byproducts: 2\n",
      "seismic: 2\n",
      "unless: 2\n",
      "rollout: 2\n",
      "malian: 2\n",
      "kerosene: 2\n",
      "shanks: 2\n",
      "offensives: 2\n",
      "lind: 2\n",
      "altruism: 2\n",
      "motivates: 2\n",
      "cause: 2\n",
      "decline: 2\n",
      "grounds: 2\n",
      "underpinning: 2\n",
      "metre: 2\n",
      "assistants: 2\n",
      "country: 2\n",
      "usmade: 2\n",
      "officers: 2\n",
      "exaggerate: 2\n",
      "astros: 2\n",
      "fox: 2\n",
      "oneshot: 2\n",
      "buchholz: 2\n",
      "greenhousegas: 2\n",
      "outweigh: 2\n",
      "coda: 2\n",
      "college: 2\n",
      "kiprusoff: 2\n",
      "courses: 2\n",
      "home: 2\n",
      "obtained: 2\n",
      "guido: 2\n",
      "gobbled: 2\n",
      "averaging: 2\n",
      "scheduling: 2\n",
      "alamos: 2\n",
      "critiques: 2\n",
      "grills: 2\n",
      "hosts: 2\n",
      "adventures: 2\n",
      "badly: 2\n",
      "vault: 2\n",
      "eurotunnel: 2\n",
      "judgements: 2\n",
      "fabricating: 2\n",
      "whatever: 2\n",
      "suzanne: 2\n",
      "chill: 2\n",
      "nearer: 2\n",
      "palestinian: 2\n",
      "deserved: 2\n",
      "myriad: 2\n",
      "coincidental: 2\n",
      "massey: 2\n",
      "openended: 2\n",
      "raking: 2\n",
      "socialize: 2\n",
      "cull: 2\n",
      "global: 2\n",
      "breeding: 2\n",
      "putin: 2\n",
      "slower: 2\n",
      "blount: 2\n",
      "boycott: 2\n",
      "cotterill: 2\n",
      "coast: 2\n",
      "stuart: 2\n",
      "websites: 2\n",
      "film: 2\n",
      "majority: 2\n",
      "go: 2\n",
      "danilo: 2\n",
      "rubble: 2\n",
      "ade: 2\n",
      "reversal: 2\n",
      "commend: 2\n",
      "cupwinning: 2\n",
      "cmbs: 2\n",
      "partied: 2\n",
      "supper: 2\n",
      "spender: 2\n",
      "politicized: 2\n",
      "chimneys: 2\n",
      "bodyguards: 1\n",
      "yearend: 1\n",
      "impressive: 1\n",
      "roundrobin: 1\n",
      "parkway: 1\n",
      "submissions: 1\n",
      "bod: 1\n",
      "performancebased: 1\n",
      "coordinator: 1\n",
      "squeamish: 1\n",
      "iranians: 1\n",
      "homes: 1\n",
      "patronage: 1\n",
      "lloyds: 1\n",
      "weaves: 1\n",
      "fbr: 1\n",
      "renditions: 1\n",
      "absolutely: 1\n",
      "immature: 1\n",
      "leave: 1\n",
      "consistency: 1\n",
      "backfired: 1\n",
      "additionally: 1\n",
      "job: 1\n",
      "derosa: 1\n",
      "alton: 1\n",
      "edit: 1\n",
      "avatars: 1\n",
      "graft: 1\n",
      "involving: 1\n",
      "noncompliance: 1\n",
      "preyed: 1\n",
      "quota: 1\n",
      "trade: 1\n",
      "sandisk: 1\n",
      "praise: 1\n",
      "hamster: 1\n",
      "determining: 1\n",
      "tibetans: 1\n",
      "ritalin: 1\n",
      "ams: 1\n",
      "melted: 1\n",
      "hibernation: 1\n",
      "poet: 1\n",
      "unaids: 1\n",
      "make: 1\n",
      "hanover: 1\n",
      "gorbachev: 1\n",
      "ithaca: 1\n",
      "showering: 1\n",
      "backroom: 1\n",
      "worm: 1\n",
      "mids: 1\n",
      "riggs: 1\n",
      "globalisation: 1\n",
      "georgia: 1\n",
      "wessex: 1\n",
      "overcrowding: 1\n",
      "whether: 1\n",
      "brady: 1\n",
      "genie: 1\n",
      "swearingin: 1\n",
      "contemplate: 1\n",
      "nominate: 1\n",
      "framing: 1\n",
      "ushers: 1\n",
      "avoiding: 1\n",
      "kilpatrick: 1\n",
      "inhome: 1\n",
      "defenseman: 1\n",
      "fronted: 1\n",
      "littleknown: 1\n",
      "legalize: 1\n",
      "national: 1\n",
      "peavy: 1\n",
      "straddling: 1\n",
      "indefinitely: 1\n",
      "legal: 1\n",
      "hula: 1\n",
      "descendants: 1\n",
      "sentence: 1\n",
      "lakeside: 1\n",
      "caddie: 1\n",
      "buying: 1\n",
      "coworkers: 1\n",
      "diocese: 1\n",
      "give: 1\n",
      "fertile: 1\n",
      "rescued: 1\n",
      "prasad: 1\n",
      "viewership: 1\n",
      "compel: 1\n",
      "orphaned: 1\n",
      "wyatt: 1\n",
      "ambush: 1\n",
      "declaring: 1\n",
      "still: 1\n",
      "john: 1\n",
      "reduce: 1\n",
      "nur: 1\n",
      "costsaving: 1\n",
      "fee: 1\n",
      "summer: 1\n",
      "nozette: 1\n",
      "pastures: 1\n",
      "drugrelated: 1\n",
      "ambrose: 1\n",
      "marketed: 1\n",
      "appraised: 1\n",
      "divisionleading: 1\n",
      "tooth: 1\n",
      "illegal: 1\n",
      "norah: 1\n",
      "cleaned: 1\n",
      "payne: 1\n",
      "reason: 1\n",
      "aerodynamic: 1\n",
      "replay: 1\n",
      "titular: 1\n",
      "tapin: 1\n",
      "hatchback: 1\n",
      "cotto: 1\n",
      "masterminds: 1\n",
      "championing: 1\n",
      "hw: 1\n",
      "helene: 1\n",
      "earhart: 1\n",
      "ico: 1\n",
      "spills: 1\n",
      "slaughtering: 1\n",
      "imprisoning: 1\n",
      "twentyfour: 1\n",
      "predisposition: 1\n",
      "bluray: 1\n",
      "cutts: 1\n",
      "week: 1\n",
      "disarming: 1\n",
      "limitededition: 1\n",
      "costner: 1\n",
      "rba: 1\n",
      "buttons: 1\n",
      "cake: 1\n",
      "free: 1\n",
      "ripa: 1\n",
      "varying: 1\n",
      "carrying: 1\n",
      "specialist: 1\n",
      "fleming: 1\n",
      "blair: 1\n",
      "ninthinning: 1\n",
      "parishioners: 1\n",
      "lego: 1\n",
      "taken: 1\n",
      "cassandra: 1\n",
      "kept: 1\n",
      "tanking: 1\n",
      "keene: 1\n",
      "constabulary: 1\n",
      "uncertainties: 1\n",
      "haddin: 1\n",
      "patrol: 1\n",
      "trotted: 1\n",
      "indict: 1\n",
      "bournemouth: 1\n",
      "nigerians: 1\n",
      "accounted: 1\n",
      "jagielka: 1\n",
      "promotion: 1\n",
      "shaping: 1\n",
      "ratified: 1\n",
      "kinnear: 1\n",
      "takeaways: 1\n",
      "twopage: 1\n",
      "chateau: 1\n",
      "lancaster: 1\n",
      "rebalancing: 1\n",
      "tijuana: 1\n",
      "headteachers: 1\n",
      "marleau: 1\n",
      "jackson: 1\n",
      "iranianamerican: 1\n",
      "capps: 1\n",
      "important: 1\n",
      "mufti: 1\n",
      "hattrick: 1\n",
      "clements: 1\n",
      "oilproducing: 1\n",
      "rip: 1\n",
      "ting: 1\n",
      "million: 1\n",
      "cardholders: 1\n",
      "cynon: 1\n",
      "realizing: 1\n",
      "indelible: 1\n",
      "nationalism: 1\n",
      "quinlan: 1\n",
      "misrepresented: 1\n",
      "blew: 1\n",
      "pests: 1\n",
      "laurence: 1\n",
      "coastguards: 1\n",
      "sucked: 1\n",
      "walsall: 1\n",
      "korea: 1\n",
      "returned: 1\n",
      "singlefamily: 1\n",
      "treading: 1\n",
      "selfsustaining: 1\n",
      "blazer: 1\n",
      "shrine: 1\n",
      "ruined: 1\n",
      "fixedterm: 1\n",
      "eavis: 1\n",
      "crawley: 1\n",
      "caseshiller: 1\n",
      "friday: 1\n",
      "licensees: 1\n",
      "insignificant: 1\n",
      "retirement: 1\n",
      "wrinkle: 1\n",
      "arod: 1\n",
      "bernier: 1\n",
      "kuszczak: 1\n",
      "intensifying: 1\n",
      "berman: 1\n",
      "cushing: 1\n",
      "demonstrating: 1\n",
      "spraying: 1\n",
      "wielding: 1\n",
      "oldfashioned: 1\n",
      "agility: 1\n",
      "motorcyclists: 1\n",
      "saul: 1\n",
      "governmentfunded: 1\n",
      "shrugged: 1\n",
      "alignment: 1\n",
      "abs: 1\n",
      "hutchinson: 1\n",
      "jt: 1\n",
      "kidnapping: 1\n",
      "monthold: 1\n",
      "atone: 1\n",
      "reacted: 1\n",
      "persisted: 1\n",
      "leung: 1\n",
      "dugan: 1\n",
      "crop: 1\n",
      "game: 1\n",
      "muslims: 1\n",
      "rebalance: 1\n",
      "laziness: 1\n",
      "putts: 1\n",
      "louis: 1\n",
      "unqualified: 1\n",
      "reintroduced: 1\n",
      "battlefields: 1\n",
      "haggerty: 1\n",
      "kiwis: 1\n",
      "abdulmutallab: 1\n",
      "mr: 1\n",
      "time: 1\n",
      "talisman: 1\n",
      "wyoming: 1\n",
      "minh: 1\n",
      "centerright: 1\n",
      "kitten: 1\n",
      "peabody: 1\n",
      "poe: 1\n",
      "payload: 1\n",
      "yearold: 1\n",
      "fugitives: 1\n",
      "bible: 1\n",
      "greens: 1\n",
      "cerny: 1\n",
      "offroad: 1\n",
      "minicamp: 1\n",
      "dunn: 1\n",
      "edwardian: 1\n",
      "instinctively: 1\n",
      "salary: 1\n",
      "shortened: 1\n",
      "finn: 1\n",
      "care: 1\n",
      "cleanenergy: 1\n",
      "granger: 1\n",
      "old: 1\n",
      "megson: 1\n",
      "driving: 1\n",
      "dispose: 1\n",
      "fast: 1\n",
      "gilroy: 1\n",
      "energized: 1\n",
      "wasp: 1\n",
      "like: 1\n",
      "oreal: 1\n",
      "jerome: 1\n",
      "pumps: 1\n",
      "downgrading: 1\n",
      "cis: 1\n",
      "spinach: 1\n",
      "consecrated: 1\n",
      "cf: 1\n",
      "venue: 1\n",
      "senatorial: 1\n",
      "determine: 1\n",
      "bellevue: 1\n",
      "crucial: 1\n",
      "sneeze: 1\n",
      "federline: 1\n",
      "reestablish: 1\n",
      "dress: 1\n",
      "cascade: 1\n",
      "longlost: 1\n",
      "dungy: 1\n",
      "repaired: 1\n",
      "retailing: 1\n",
      "mustwin: 1\n",
      "kimoon: 1\n",
      "mrsa: 1\n",
      "soto: 1\n",
      "fleury: 1\n",
      "secretaries: 1\n",
      "discontinue: 1\n",
      "russo: 1\n",
      "attending: 1\n",
      "eastleading: 1\n",
      "portraying: 1\n",
      "barbershop: 1\n",
      "fissile: 1\n",
      "brewing: 1\n",
      "karnataka: 1\n",
      "cancer: 1\n",
      "ncis: 1\n",
      "sacrifices: 1\n",
      "singh: 1\n",
      "hunk: 1\n",
      "voterigging: 1\n",
      "weeds: 1\n",
      "substantiate: 1\n",
      "omar: 1\n",
      "broth: 1\n",
      "policymaking: 1\n",
      "prejudices: 1\n",
      "geese: 1\n",
      "steve: 1\n",
      "duvall: 1\n",
      "egos: 1\n",
      "parties: 1\n",
      "blessings: 1\n",
      "napa: 1\n",
      "expedited: 1\n",
      "pristine: 1\n",
      "heaving: 1\n",
      "headfirst: 1\n",
      "handout: 1\n",
      "extradited: 1\n",
      "plans: 1\n",
      "piping: 1\n",
      "converters: 1\n",
      "fiveyear: 1\n",
      "poster: 1\n",
      "compare: 1\n",
      "pythons: 1\n",
      "compilation: 1\n",
      "steamboat: 1\n",
      "unprecedented: 1\n",
      "eastenders: 1\n",
      "eroshevich: 1\n",
      "messiah: 1\n",
      "fragrance: 1\n",
      "reneging: 1\n",
      "lawns: 1\n",
      "wiesenthal: 1\n",
      "hutton: 1\n",
      "little: 1\n",
      "skimmed: 1\n",
      "incisive: 1\n",
      "sosa: 1\n",
      "markets: 1\n",
      "frustration: 1\n",
      "want: 1\n",
      "ironic: 1\n",
      "conscience: 1\n",
      "werth: 1\n",
      "gamecocks: 1\n",
      "sightings: 1\n",
      "stevens: 1\n",
      "traffickers: 1\n",
      "commercials: 1\n",
      "rutland: 1\n",
      "corrugated: 1\n",
      "dec: 1\n",
      "zealand: 1\n",
      "reconciliation: 1\n",
      "britannia: 1\n",
      "orchids: 1\n",
      "pavarotti: 1\n",
      "alberto: 1\n",
      "snowmobile: 1\n",
      "glasses: 1\n",
      "early: 1\n",
      "undervalues: 1\n",
      "milan: 1\n",
      "lovely: 1\n",
      "luhrmann: 1\n",
      "pecking: 1\n",
      "openers: 1\n",
      "beefy: 1\n",
      "shalom: 1\n",
      "shampoo: 1\n",
      "relaunched: 1\n",
      "could: 1\n",
      "foye: 1\n",
      "plastic: 1\n",
      "junior: 1\n",
      "bottling: 1\n",
      "westside: 1\n",
      "avi: 1\n",
      "ovaries: 1\n",
      "blacklist: 1\n",
      "carcass: 1\n",
      "preserves: 1\n",
      "theoretical: 1\n",
      "family: 1\n",
      "money: 1\n",
      "despite: 1\n",
      "smears: 1\n",
      "illegitimate: 1\n",
      "dodgy: 1\n",
      "landis: 1\n",
      "instructive: 1\n",
      "unguarded: 1\n",
      "denounce: 1\n",
      "entertained: 1\n",
      "souvenir: 1\n",
      "windmills: 1\n",
      "mid: 1\n",
      "us: 1\n",
      "madeline: 1\n",
      "shootouts: 1\n",
      "evened: 1\n",
      "inhibited: 1\n",
      "quibble: 1\n",
      "hastily: 1\n",
      "sonoma: 1\n",
      "hefner: 1\n",
      "tyra: 1\n",
      "debtor: 1\n",
      "hewlettpackard: 1\n",
      "underscored: 1\n",
      "verbal: 1\n",
      "bermuda: 1\n",
      "digit: 1\n",
      "lorry: 1\n",
      "barely: 1\n",
      "whims: 1\n",
      "condolences: 1\n",
      "graces: 1\n",
      "appalachian: 1\n",
      "voluptuous: 1\n",
      "postponed: 1\n",
      "sammy: 1\n",
      "stage: 1\n",
      "mulling: 1\n",
      "encryption: 1\n",
      "ac: 1\n",
      "kampala: 1\n",
      "mouthwatering: 1\n",
      "trimming: 1\n",
      "denunciations: 1\n",
      "taiwan: 1\n",
      "cram: 1\n",
      "leicester: 1\n",
      "well: 1\n",
      "protracted: 1\n",
      "appears: 1\n",
      "slim: 1\n",
      "spans: 1\n",
      "reluctance: 1\n",
      "representative: 1\n",
      "show: 1\n",
      "melting: 1\n",
      "resigned: 1\n",
      "stacey: 1\n",
      "mcenroe: 1\n",
      "dem: 1\n",
      "mindy: 1\n",
      "ballooning: 1\n",
      "journalists: 1\n",
      "foreignlanguage: 1\n",
      "pedrosa: 1\n",
      "wreaking: 1\n",
      "exposing: 1\n",
      "londonderry: 1\n",
      "plied: 1\n",
      "evolved: 1\n",
      "crime: 1\n",
      "retrieved: 1\n",
      "johannesburg: 1\n",
      "tilting: 1\n",
      "mace: 1\n",
      "mirroring: 1\n",
      "bunning: 1\n",
      "canada: 1\n",
      "certainly: 1\n",
      "henri: 1\n",
      "jericho: 1\n",
      "carswell: 1\n",
      "havens: 1\n",
      "venison: 1\n",
      "stronger: 1\n",
      "hinduism: 1\n",
      "washingtonbased: 1\n",
      "landslides: 1\n",
      "panoramic: 1\n",
      "portray: 1\n",
      "deborah: 1\n",
      "lower: 1\n",
      "ginobili: 1\n",
      "deem: 1\n",
      "laundry: 1\n",
      "licensing: 1\n",
      "mwai: 1\n",
      "pienaar: 1\n",
      "league: 1\n",
      "even: 1\n",
      "people: 1\n",
      "business: 1\n",
      "stories: 1\n",
      "burlesque: 1\n",
      "tempted: 1\n",
      "five: 1\n",
      "lorna: 1\n",
      "garbage: 1\n",
      "laporte: 1\n",
      "accompanies: 1\n",
      "java: 1\n",
      "washington: 1\n",
      "boys: 1\n",
      "forgetting: 1\n",
      "highestranked: 1\n",
      "prefer: 1\n",
      "quicken: 1\n",
      "reverend: 1\n",
      "cumbria: 1\n",
      "miley: 1\n",
      "panetta: 1\n",
      "ache: 1\n",
      "concealed: 1\n",
      "ports: 1\n",
      "banks: 1\n",
      "never: 1\n",
      "locate: 1\n",
      "courtrooms: 1\n",
      "dermot: 1\n",
      "disclosure: 1\n",
      "de: 1\n",
      "inmate: 1\n",
      "step: 1\n",
      "commentator: 1\n",
      "enough: 1\n",
      "wedge: 1\n",
      "chatter: 1\n",
      "chicago: 1\n",
      "ladies: 1\n",
      "back: 1\n",
      "many: 1\n",
      "contradictory: 1\n",
      "bloodletting: 1\n",
      "pond: 1\n",
      "mccartney: 1\n",
      "occupying: 1\n",
      "boxers: 1\n",
      "haney: 1\n",
      "reimbursed: 1\n",
      "help: 1\n",
      "palpable: 1\n",
      "plummeting: 1\n",
      "writedowns: 1\n",
      "hernandez: 1\n",
      "representation: 1\n",
      "annex: 1\n",
      "zabaleta: 1\n",
      "blackout: 1\n",
      "powertrain: 1\n",
      "bouton: 1\n",
      "dimon: 1\n",
      "shrug: 1\n",
      "terms: 1\n",
      "far: 1\n",
      "los: 1\n",
      "myrtle: 1\n",
      "hantuchova: 1\n",
      "landmarks: 1\n",
      "inaba: 1\n",
      "reneged: 1\n",
      "starting: 1\n",
      "according: 1\n",
      "meals: 1\n",
      "tantrums: 1\n",
      "highrisk: 1\n",
      "probed: 1\n",
      "casings: 1\n",
      "lotteries: 1\n",
      "pyrenees: 1\n",
      "accomplishments: 1\n",
      "rescind: 1\n",
      "yank: 1\n",
      "powerhouse: 1\n",
      "reckoning: 1\n",
      "outlet: 1\n",
      "adequately: 1\n",
      "jeanpaul: 1\n",
      "know: 1\n",
      "dewey: 1\n",
      "twickenham: 1\n",
      "thursday: 1\n",
      "workouts: 1\n",
      "trampoline: 1\n",
      "lots: 1\n",
      "designed: 1\n",
      "respite: 1\n",
      "hell: 1\n",
      "landry: 1\n",
      "advocates: 1\n",
      "censorship: 1\n",
      "screwing: 1\n",
      "rusty: 1\n",
      "divinity: 1\n",
      "sept: 1\n",
      "shrift: 1\n",
      "delight: 1\n",
      "liverpool: 1\n",
      "let: 1\n",
      "sensibility: 1\n",
      "amplify: 1\n",
      "dwayne: 1\n",
      "prostitute: 1\n",
      "sculptor: 1\n",
      "height: 1\n",
      "scraps: 1\n",
      "elbaradei: 1\n",
      "tastes: 1\n",
      "untreated: 1\n",
      "opinionated: 1\n",
      "superdome: 1\n",
      "wall: 1\n",
      "makeover: 1\n",
      "generation: 1\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "word_counts = Counter(words)\n",
    "sorted_words = word_counts.most_common()\n",
    "for word, count in sorted_words:\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53a1eca-3e73-4ad1-a9e5-1f7343b21d9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
